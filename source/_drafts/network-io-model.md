---
title: IO多路复用
mathjax: false
tags: IO多路复用
categories: 网络
---



## 序

![IO模型](https://cdn.jsdelivr.net/gh/Winniekun/cloudImg@master/uPic/image-20210605211455238.png)







![函数调用关系](https://cdn.jsdelivr.net/gh/Winniekun/cloudImg@master/uPic/image-20210605211529353.png)



## 阻塞IO

也就是正常的Socket编程流程，其中，共有两个**阻塞点儿**：

1. accept
   - 进行网络连接
2. read()/write() 步骤
   - 数据从网络阶段复制到内核空间
   - 从内核空间复制用户空间

整体流程如下图所示：

![阻塞IO](https://cdn.jsdelivr.net/gh/Winniekun/cloudImg@master/uPic/%E9%98%BB%E5%A1%9EIO.png)

所以，如果这个连接的主动方一直不发数据，那么服务端线程将会一直阻塞在 read 函数上不返回，也无法接受其他客户端连接。

## 非阻塞IO

除去accept这个客观原因，能够优化的就是read函数

**小优化：**

每次链接之后，fork一个子进程去进行业务处理，也就是调用read函数，然后父进程依旧可以不停的去建立新的链接。而不用等待新的上一次的链接处理完了，才能接受新的请求。这种属于从开发的角度就行优化，只是使用了多进程的思路，避免了主进程阻塞在read函数中。

对于真正的非阻塞IO，肯定不是通过用户层面的优化处理来解决的，而是真正的需要一种read函数，是非阻塞形式的。能够实时的回馈当前的状态，然后主进程能够通过当前状态，做出相应的处理。

操作系统提供了这样的功能，只需要在调用 read 前，将文件描述符设置为非阻塞即可。这样，就需要用户进程循环调用 read，直到返回值不为 -1，再开始处理业务。

![非阻塞IO](https://cdn.jsdelivr.net/gh/Winniekun/cloudImg@master/uPic/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.png)

根据图示，在数据还未完全传输到内核内核缓存区时，这个阶段时非阻塞的（也就是我们说的read步骤的第一点儿），但是第二点，也就是从内核缓冲区复制到用户缓冲区，仍然是阻塞的。

根据这种机制，我们可以将多个accept之后的文件描述符同一个放到一个集合中，然后弄一个新的进程去不断遍历这个数组，调用每一个元素的非阻塞 read 方法。虽然是优化了不少，但是我们仍然是执行N次的系统调用，仍然有很大的浪费。

那么能不能让操作系统提供给我们一个有这样效果的函数，我们将一批文件描述符通过一次系统调用传给内核，由内核层去遍历，才能真正解决这个问题。

## IO多路复用

实质目标：同时检查多个文件描述符，查看它们是否准备好了执行I/O操作（或者说看I/O系统调用是否可以非阻塞的执行）。

文件描述符的状态转化：通过一些I/O事件触发。譬如输入数据到达等

### 触发条件

文件描述符就绪的通知模式

#### 水平触发

任意时刻重复检查I/O状态，没有必要每次当文件描述符就绪后需要尽可能多的执行IO操作。

#### 边缘触发

只有当I/O事件发生时，我们才会收到通知。在另一个事件到来之前不会收到任何新的通知。同时当文件描述符收到I/O事件通知之后，并不知道要处理多少I/O。

所以需要遵循以下的规则：

1. 收到一个I/O事件之后，应该在响应的文件描述符上尽可能多的执行I/O。如果没有这么做，可能就会失去执行I/O的机会。
2. 被检查的文件描述符应该设置为非阻塞式模式。在得到I/O事件通知后重复执行I/O操作。

### select

select 是操作系统提供的系统调用函数，通过它，我们可以把一个文件描述符的数组发给操作系统， 让操作系统去遍历，确定哪个文件描述符可以读写， 然后告诉我们去处理。

因为每次accept之后，内核会返回一个socket的文件描述符，我们可以每 accept 一个客户端连接后，将这个文件描述符（connfd）放到一个数组里。然后使用select系统调用，将这个数组一起放入到内核中，然后让内核去遍历，标识出哪些文件可读、可写，然后再返回给我们这个集合。我们依然需要遍历刚刚提交给操作系统的 list。只不过，操作系统会将准备就绪的文件描述符做上标识，用户层将不会再有无意义的系统调用开销。

![640](https://cdn.jsdelivr.net/gh/Winniekun/cloudImg@master/uPic/640.gif)

可以看出几个细节：

1. select 调用需要传入 fd 数组，需要拷贝一份到内核，高并发场景下这样的拷贝消耗的资源是惊人的。（可优化为不复制）

2. select 在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，是个同步过程，只不过无系统调用切换上下文的开销。（内核层可优化为异步事件通知）

3. select 仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。（可优化为只返回给用户就绪的文件描述符，无需用户做无效的遍历）



![image-20210605234034863](https://cdn.jsdelivr.net/gh/Winniekun/cloudImg@master/uPic/image-20210605234034863.png)

可以看到，这种方式，既做到了一个线程处理多个客户端连接（文件描述符），又减少了系统调用的开销（多个文件描述符只有一次 select 的系统调用 + n 次就绪状态的文件描述符的 read 系统调用）。

### poll

它和 select 的主要区别就是，去掉了 select 只能监听 1024 个文件描述符的限制。因为它使用的是链表，而不是一个数组。

### epoll

select的三个细节：

1. select 调用需要传入 fd 数组，需要拷贝一份到内核，高并发场景下这样的拷贝消耗的资源是惊人的。（可优化为不复制）

2. select 在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，是个同步过程，只不过无系统调用切换上下文的开销。（内核层可优化为异步事件通知）

3. select 仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。（可优化为只返回给用户就绪的文件描述符，无需用户做无效的遍历）

所以 epoll 主要就是针对这三点进行了改进。

1. 内核中保存一份文件描述符集合，无需用户每次都重新传入，只需告诉内核修改的部分即可。

2. 内核不再通过轮询的方式找到就绪的文件描述符，而是通过异步 IO 事件唤醒。

3. 内核仅会将有 IO 事件的文件描述符返回给用户，用户也无需遍历整个文件描述符集合。

**epoll_create**

```C
int epoll_create(int size);

**epoll_ctl**

​```c
int epoll_ctl(
  int epfd, int op, int fd, struct epoll_event *event);
```

**epoll_wait**

```c
int epoll_ctl(
  int epfd, int op, int fd, struct epoll_event *event);
```

![epoll](https://cdn.jsdelivr.net/gh/Winniekun/cloudImg@master/uPic/epoll.gif)

一切的开始，都起源于这个 read 函数是操作系统提供的，而且是阻塞的，我们叫它 **阻塞 IO**。

为了破这个局，程序员在用户态通过多线程来防止主线程卡死。

后来操作系统发现这个需求比较大，于是在操作系统层面提供了非阻塞的 read 函数，这样程序员就可以在一个线程内完成多个文件描述符的读取，这就是 **非阻塞 IO**。

但多个文件描述符的读取就需要遍历，当高并发场景越来越多时，用户态遍历的文件描述符也越来越多，相当于在 while 循环里进行了越来越多的系统调用。

后来操作系统又发现这个场景需求量较大，于是又在操作系统层面提供了这样的遍历文件描述符的机制，这就是 **IO 多路复用**。

多路复用有三个函数，最开始是 select，然后又发明了 poll 解决了 select 文件描述符的限制，然后又发明了 epoll 解决 select 的三个不足。

------

所以，IO 模型的演进，其实就是时代的变化，倒逼着操作系统将更多的功能加到自己的内核而已。

## Redis高性能原因之---IO复用

通过上述的分析，提高网络通讯的方式之一---尽可能的减少阻塞点儿的阻塞时间。

![image](/Users/weikunkun/Library/Application Support/typora-user-images/image-20210423143252526.png)

对于Redis而言，同理。



## references

- [你管这破玩意儿叫IO多路复用](https://mp.weixin.qq.com/s/YdIdoZ_yusVWza1PU7lWaw)
- Linux/Unix系统编程手册

